<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Agent Laboratory: Using LLMs as Research Assistants">
  <meta name="keywords" content="automated research, LLM research, LLM coding, autoML, autonomous research LLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Agent Laboratory</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" integrity="sha512-iBBXm8fW90+nuLcSKlbmrPcLa0OT92xO1BIsZ+ywDWZCvqsWgccV3gFoRBv0z+8dLJgyAHIhR35VZc2oM/gI1w==" crossorigin="anonymous" referrerpolicy="no-referrer" /> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link rel="icon" href="./favicon.ico">

  <meta property="og:site_name" content="Agent Laboratory: Using LLMs as Research Assistants" />
  <meta property="og:type" content="video.other" />
  <meta property="og:title" content="Agent Laboratory: Using LLMs as Research Assistants" />
  <meta property="og:description" content="by Samuel Schmidgall at JHU" />
  <meta property="og:url" content="https://agentlaboratory.github.io/" />
  <meta property="og:image" content="https://agentlaboratory.github.io/static/images/preview_srt.gif" />
  <meta property="og:image:secure" content="https://agentlaboratory.github.io/static/images/preview_srt.gif" />
  <meta property="og:video" content="" />
  <meta property="og:video:secure" content="" />
  <meta property="og:image:width" content="1280" />
  <meta property="og:image:height" content="720" />

  <meta property="article:publisher" content="https://agentlaboratory.github.io" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Agent Laboratory: Using LLMs as Research Assistants" />
  <meta name="twitter:description" content="by Samuel Schmidgall at JHU " />
  <meta name="twitter:url" content="https://agentlaboratory.github.io/" />
  <meta name="twitter:image" content="https://agentlaboratory.github.io/static/images/preview_srt.gif" />
  <meta name="twitter:site" content="@SRSchmidgall" />
  <meta name="twitter:card" content="player" />
  <meta name="twitter:player" content="" />
  <meta name="twitter:player:width" content="1280" />
  <meta name="twitter:player:height" content="720" />

  <script src="https://www.youtube.com/iframe_api"></script>
</head>


<body>

<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-two-thirds is-centered has-text-centered">
          <h1 class="title is-1 publication-title">Agent Laboratory:</h1>
          <h2 class="subtitle is-2 publication-subtitle">Using LLM Agents as Research Assistants</h2>
          <div style="display: flex; justify-content: center; gap: 20px; margin-bottom: 30px;">
            <img src="./static/images/amd_jhu_logo.png" alt="Primary Logo" style="width: 45%;height: 90%">
          </div>
          <h2 class="subtitle is-5 publication-subtitle">Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, and Emad Barsoum</h2>

            <div class="publication-links">
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2501.04227"
                   class="external-link button is-normal is-rounded is-dark" target="_blank">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/SamuelSchmidgall/AgentLaboratory" class="external-link button is-normal is-rounded is-dark" target="_blank">
                  <span class="icon">
                      <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
                  </span>
                  <span>Code</span>
                </a>
              </span>
              
              <!--span class="link-block">
                <a href="#youtube-video" class="external-link button is-normal is-rounded is-dark">
                  <span class="fab fa-youtube">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>&nbsp Video</span>
                </a>
              </span-->
              <span class="link-block">
                <a href="#citation-ref" class="external-link button is-normal is-rounded is-dark">
                  <span class="fa fa-paper-plane">
                    <i class="fa fa-paper-plane"></i>
                  </span>
                  <span>&nbsp Citation</span>
                </a>
              </span>
              <span class="link-block">
                <a href="resources/imgs/examplepaper.pdf" class="external-link button is-normal is-rounded is-dark">
                  <span class="fas fa-file-alt">
                     <i class='fas fa-file-alt'></i>
                  </span>
                  <span>&nbsp Example</span>
                </a>
              </span>
            </div>
          </div>
        </div>
        <div class="columns is-centered">
          <div class="column is-two-thirds has-text-centered">
            <img src="resources/imgs/Agent-Laboratory.png" alt="Descriptive text about the image" style="max-width: 100%; height: auto;">
          </div>
        </div>
      </div>
    </div>
  </section>

  



<!--section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-2">Video Introduction</h2>
          <div id="youtube-video" class="content has-text-justified">
            <iframe id="youtube-video-frame" src="https://www.youtube.com/embed/OO2kPK5-qno"></iframe>
            </iframe>
          </div>
      </div>
    </div>
  </div>
</section-->

<section class="section">
  <div class="container">
    <h2 class="title is-2" style="text-align: center;"></h2> 
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <div class="content has-text-justified">
          <h4 class="title is-4" style="margin-bottom: 1rem;">What does Agent Laboratory do for you?</h4>
          <p style="padding-bottom: 30px;">
            <kbd>Agent Laboratory</kbd> takes as input a human-produced research idea and outputs a research report and code repository. First and foremost, <kbd>Agent Laboratory</kbd> is meant to assist you as the human researcher toward implementing your research ideas. You are the pilot. <kbd>Agent Laboratory</kbd> provides a structured framework that adapts to your computational resources, whether you're running it on a MacBook or on a GPU cluster. Agent Laboratory consists of specialized agents driven by large language models to support you through the entire research workflowâ€”from conducting literature reviews and formulating plans to executing experiments and writing comprehensive reports. This system is not designed to replace your creativity but to complement it, <strong>enabling you to focus on ideation and critical thinking while automating repetitive and time-intensive tasks like coding and documentation</strong>. By accommodating varying levels of computational resources and human involvement, <kbd>Agent Laboratory</kbd> aims to accelerate scientific discovery and optimize your research productivity.
          </p>
          <h4 class="title is-4" style="margin-bottom: 1rem;">How does Agent Laboratory work?</h4>
          
          
          <p style="padding-bottom: 50px;">
            <kbd>Agent Laboratory</kbd> consists of three primary phases that systematically guide the research process: (1) Literature Review, (2) Experimentation, and (3) Report Writing. During each phase, specialized agents driven by LLMs collaborate to accomplish distinct objectives, integrating external tools like arXiv, Hugging Face, Python, and LaTeX to optimize outcomes. This structured workflow begins with the independent collection and analysis of relevant research papers, progresses through collaborative planning and data preparation, and results in automated experimentation and comprehensive report generation. Details on specific agent roles and their contributions across these phases are discussed in the paper. The modular design ensures compute flexibility, accommodating diverse resource availability while maintaining efficiency in generating high-quality research artifacts.
          </p>
          <!-- Example of how to insert an image -->
          <img src="resources/imgs/Agent-Lab-Workflow.png" alt="Descriptive text about the image" style="max-width: 100%; padding-bottom: 50px; height: auto;">
          <!-- You can add more <img> tags as needed -->
          <h4 class="title is-4" style="margin-bottom: 1rem;">Solving ML problems</h4>
          <p style="padding-bottom: 6px;">
            The first step toward performing research is building the capability for solving ML problems. <kbd>Agent Laboratory</kbd> addresses this through the mle-solver. This tool acts as a general purpose ML code solver, taking in research directions from previous phases as text and iteratively improving research code. To accomplish this, a collection of top programs are iteratively conditioned on inputs like task instructions, command descriptions, and distilled knowledge in order to improve the experimental results according to a scoring function. A set of changes are generated via two commands: REPLACE, which rewrites all code, and EDIT, which modifies specific lines. Successfully compiled code updates top programs based on scores, while errors prompt up to three repair attempts before trying new code. The agent reflects on each step to refine outcomes.
          </p>
          <!-- Example of how to insert an image -->

          <div style="text-align: center;">
            <img src="resources/imgs/mle-solver.png" alt="Descriptive text about the image" style="max-width: 80%; padding-bottom: 50px; height: auto;">
          </div>

          <p style="padding-bottom: 6px;">
            <strong>MLE-Bench Evaluation:</strong> In order to assess the effectiveness of mle-solver independent of the full <kbd>Agent Laboratory</kbd> workflow, we evaluate it in isolation on a subset of 10 ML challenges from <a href="https://openai.com/index/mle-bench/">MLE-bench</a>. MLE-bench is a benchmark designed to assess the capability of agents in handling real-world ML tasks on Kaggle competitions. This benchmark compares agent performance with human baselines, scoring agents with Kaggleâ€™s medal system, and incorporating mechanisms to mitigate contamination and plagiarism risks. We find that Agent Laboratoryâ€™s mle-solver is more consistently high scoring than other solvers, with mle-solver obtaining four medals (two gold, one silver, and one bronze) compared with OpenHands (gpt-4o) obtaining two medals (two gold), AIDE (o1-preview) obtaining two medals (one gold, one bronze) and MLAB obtaining zero medals. Additionally, mle-solver obtained above median human performance on six out of ten benchmarks, with AIDE obtaining five out of ten, OpenHands two out of ten, and MLAB zero out of ten.
          </p>
          <img src="resources/imgs/mlebench_graph_full.png" alt="Descriptive text about the image" style="max-width: 100%; padding-bottom: 50px; height: auto;">
          <!-- You can add more <img> tags as needed -->
          <h4 class="title is-4" style="margin-bottom: 1rem;">Writing research reports</h4>
          <p style="padding-bottom: 8px;">
            The second step is generating a research report based on the experiment design and results. For this we introduce the paper-solver, which focuses on report generation. This module acts as a results and code-to-report generator, summarizing the outputs and findings from previous experimental phases into a human-readable academic paper. paper-solver synthesizes research from previous stages, providing researchers with a clear summary of accomplishments. Inputs include the research plan, experimental results, derived insights, and a literature review, producing outputs in the standard academic paper format suitable for conference submissions.
          </p>
          <!-- Example of how to insert an image -->
          <img src="resources/imgs/paper_workflow_new.png" alt="Descriptive text about the image" style="max-width: 100%; padding-bottom: 50px; height: auto;">
          <!-- You can add more <img> tags as needed -->
          <h4 class="title is-4" style="margin-bottom: 1rem;">Human perception of quality by language model</h4>
          <p style="padding-bottom: 8px;">
            This study evaluated the human-perceived quality of research outputs generated by three language model backendsâ€”gpt-4o, o1-mini, and o1-previewâ€”along three dimensions: experimental quality, report quality, and perceived usefulness. Using five research questions as templates, 15 papers were autonomously generated by Agent Laboratory and subsequently reviewed by 10 volunteer PhD students. These reviewers assessed the outputs on a scale of 1 to 5. Results revealed that o1-preview delivered the highest perceived usefulness (4.4/5) and report quality (3.4/5), though its experimental quality was slightly lower (2.9/5). The o1-mini model achieved the highest experimental quality score (3.2/5), with consistent performance across the other two dimensions. In contrast, gpt-4o scored lowest overall, particularly in experimental quality (2.6/5), though it retained a relatively strong usefulness rating of 4.0/5. These findings indicate notable differences in how well each backend aligns with researcher expectations for autonomous research generation.
          </p>
          <img src="resources/imgs/papers.png" alt="Descriptive text about the image" style="max-width: 100%; padding-bottom: 50px; height: auto;">

          <p>The analysis further revealed that the perceived quality varied based on research topics. For instance, the word order topic received the highest average report quality (3.8/5) and usefulness (4.5/5) but shared the lowest experimental quality rating (2.7/5) with the image noise topic. The cognitive bias topic achieved the highest experimental quality (3.2/5). Variability was particularly pronounced in the image noise topic, where gpt-4o scored poorly (1.5/5 for experimental quality and 2.5/5 for usefulness) while o1-mini excelled (4.0/5 and 4.5/5, respectively).</p>


          <h4 class="title is-4" style="margin-bottom: 1rem;">Human reviewer scores by language model</h4>
          <p style="padding-bottom: 8px;">
            Human reviewers evaluated papers generated by Agent Laboratory using NeurIPS-style criteria, assessing quality, significance, clarity, soundness, presentation, and contribution. Across all metrics, o1-preview achieved the highest average overall score (4.0/10), followed by o1-mini (3.8/10) and gpt-4o (3.5/10). While o1-mini excelled in quality with a score of 2.3/4 and o1-preview led in soundness (2.2/4), all models showed modest performance in significance (2.2â€“2.5/4) and contribution (average 2.1/4), highlighting limited originality and impact. Clarity scores varied slightly, with gpt-4o rated highest (2.6/4). Despite these differences, all models fell well below the average score of 5.9 for accepted NeurIPS papers, indicating substantial gaps in technical and methodological rigor. These findings emphasize the need for further refinement of Agent Laboratory to align with the standards of high-quality research publications.
          </p>

          <div style="text-align: center;">
            <img src="resources/imgs/scores.png" alt="Descriptive text about the image" style="max-width: 80%; padding-bottom: 50px; height: auto;">
          </div>



          <h4 class="title is-4" style="margin-bottom: 1rem;">Co-Pilot Quality</h4>
          <p style="padding-bottom: 8px;">
            We next evaluate Agent Laboratory in co-pilot (human-guided) mode. Researchers rated the tool on utility, continuation likelihood, satisfaction, and usability, scoring it 3.5/5, 3.75/5, 3.63/5, and 4.0/5, respectively, across custom and preselected topics. Custom topics generally received higher scores, particularly in utility (+0.5), continuation (+0.5), and satisfaction (+0.25), though usability was rated marginally lower (-0.5). Preselected topics performed better in external evaluations, contrasting with the higher scores custom topics received in self-assessments. In assessing paper quality, co-pilot scores improved compared to the autonomous mode, with an average overall score increase from 3.8/10 to 4.38/10 (+0.58). Gains were observed in quality (+0.75), clarity (+0.23), soundness (+0.48), and presentation (+0.33), though minimal changes or decreases were noted in significance (-0.05) and contribution (+0.03).
          </p>

          <div style="text-align: center;">
            <img src="resources/imgs/copilot_evals.png" alt="Descriptive text about the image" style="max-width: 80%; padding-bottom: 50px; height: auto;">
          </div>


          <h4 class="title is-4" style="margin-bottom: 1rem;">Runtime Statistics</h4>
          <p style="padding-bottom: 8px;">
            The runtime and cost analysis of Agent Laboratory revealed that gpt-4o was the most computationally efficient and cost-effective model backend, completing the entire workflow in 1165.4 seconds at a cost of $2.33, significantly outperforming o1-mini and o1-preview. o1-mini and o1-preview required 3616.8 and 6201.3 seconds, respectively, with costs of $7.51 and $13.10 per workflow. gpt-4o excelled in speed and cost across key subtasks, including Running Experiments and Report Writing, where it was 3â€“5x faster and far cheaper than its counterparts. Despite these differences, all models achieved high reliability, with gpt-4o recording a 98.5% success rate and o1-mini and o1-preview achieving 95.7%. Report Writing emerged as the most expensive phase, particularly for o1-preview, which incurred a cost of $9.58 for this task alone. 
          </p>
          <div style="text-align: center;">
            <img src="resources/imgs/stats.png" alt="Descriptive text about the image" style="max-width: 80%; padding-bottom: 50px; height: auto;">
          </div>
          
          <h4 class="title is-4" style="margin-bottom: 1rem;">Related Work</h4>
          <p style="padding-bottom: 8px;">
            In this section, I hope to highlight many of the amazing related works being done other groups. <a href="https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full.pdf">The Virtual Lab</a> introduces a team of LLM agents working as scientists alongside a human researcher who provides high-level feedback, with the end result being novel nanobody binders aimed at addressing recent variants of SARS-CoV-2. <a href="https://www.nature.com/articles/s42256-024-00832-8">ChemCrow</a> and Coscientist <a href="https://www.nature.com/articles/s41586-023-06792-0">Coscientist</a> demonstrate the ability for autonomous ideation and experimentation in chemistry. <a href="https://arxiv.org/pdf/2404.07738">ResearchAgent</a> automates research idea generation, experiment design, and iterative refinement using feedback from reviewing agents aligned with human evaluation criteria. <a href="https://arxiv.org/pdf/2408.06292">The AI Scientist</a> extends this automation to encompass end-to-end scientific discovery, including coding, experiment execution, and automated peer review for manuscript generation. Despite these advancements, the paper <a href="https://arxiv.org/pdf/2409.04109">"Can LLMs Generate Novel Research Ideas?"</a> highlight limitations in the feasibility and implementation details of LLM ideation, indicating a complementary rather than replacement role for LLMs in research. We hope to see many more exciting works in autonomous research and hope that <kbd>Agent Laboratory</kbd> can help you with your own research!
          </p>

        </div>

      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div id="citation-ref" class="container content">
    <h2 class="titile">BibTeX</h2>
    <pre><code>@misc{schmidgall2025agentlaboratoryusingllm,
      title={Agent Laboratory: Using LLM Agents as Research Assistants}, 
      author={Samuel Schmidgall and Yusheng Su and Ze Wang and Ximeng Sun and Jialian Wu and Xiaodong Yu and Jiang Liu and Zicheng Liu and Emad Barsoum},
      year={2025},
      eprint={2501.04227},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2501.04227},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>Page template borrowed from <a href="https://nerfies.github.io/"><span class="dnerf">Nerfies</span>
    </div>
  </div>
</footer>

</body>
</html>
